# VPBank AI Credit Score Infrastructure# VPBank AI Enhanced Credit Score Infrastructure



Háº¡ táº§ng AWS AI/ML cho há»‡ thá»‘ng cháº¥m Ä‘iá»ƒm tÃ­n dá»¥ng tá»± Ä‘á»™ng sá»­ dá»¥ng Terraform.This Terraform project creates a complete AWS infrastructure for an AI-enhanced credit scoring system for the VPBank Hackathon.



## ğŸ“‹ Má»¥c lá»¥c## Architecture Overview



- [Tá»•ng quan](#tá»•ng-quan)The infrastructure includes:

- [Kiáº¿n trÃºc há»‡ thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)

- [YÃªu cáº§u](#yÃªu-cáº§u)### Core Components

- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)- **VPC with Public/Private Subnets**: Secure network architecture across multiple AZs

- [Cáº¥u trÃºc thÆ° má»¥c](#cáº¥u-trÃºc-thÆ°-má»¥c)- **Application Load Balancer**: High availability and traffic distribution

- [HÆ°á»›ng dáº«n sá»­ dá»¥ng](#hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)- **Auto Scaling Group**: Automatic scaling based on demand

- [CÃ¡c dá»‹ch vá»¥ AWS](#cÃ¡c-dá»‹ch-vá»¥-aws)- **RDS MySQL Database**: Secure, managed database for customer and transaction data

- [Pipeline ML](#pipeline-ml)- **ElastiCache Redis**: In-memory caching for improved performance

- [Giao diá»‡n ngÆ°á»i dÃ¹ng](#giao-diá»‡n-ngÆ°á»i-dÃ¹ng)

- [Troubleshooting](#troubleshooting)### AI/ML Components

- **SageMaker Notebook**: For model development and training

## ğŸ¯ Tá»•ng quan- **Lambda Function**: Real-time credit score inference API

- **S3 Bucket**: Storage for ML models and training data

Há»‡ thá»‘ng AI/ML hoÃ n chá»‰nh Ä‘á»ƒ cháº¥m Ä‘iá»ƒm tÃ­n dá»¥ng khÃ¡ch hÃ ng dá»±a trÃªn 3 loáº¡i dá»¯ liá»‡u:- **API Gateway**: RESTful API endpoint for credit scoring



- **Traditional Data**: ThÃ´ng tin tÃ i chÃ­nh truyá»n thá»‘ng (thu nháº­p, tÃ i sáº£n, ná»£)### Security & Monitoring

- **Transaction Data**: Lá»‹ch sá»­ giao dá»‹ch ngÃ¢n hÃ ng- **Security Groups**: Network-level security controls

- **Social Data**: Dá»¯ liá»‡u máº¡ng xÃ£ há»™i vÃ  hÃ nh vi ngÆ°á»i dÃ¹ng- **IAM Roles & Policies**: Fine-grained access control

- **Secrets Manager**: Secure storage of database credentials

Há»‡ thá»‘ng tá»± Ä‘á»™ng thu tháº­p dá»¯ liá»‡u, xá»­ lÃ½ ETL, huáº¥n luyá»‡n model, vÃ  cung cáº¥p API Ä‘á»ƒ tra cá»©u Ä‘iá»ƒm tÃ­n dá»¥ng.- **CloudWatch**: Logging and monitoring



## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng## Prerequisites



```1. **AWS Account**: With appropriate permissions to create resources

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”2. **Terraform**: Version >= 1.0 installed

â”‚   S3 Raw    â”‚  â† Upload dá»¯ liá»‡u thÃ´ (CSV)3. **AWS CLI**: Configured with your credentials

â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜4. **Git**: For version control

       â”‚ trigger

       â–¼## Quick Start

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ AWS Glue    â”‚  â† ETL: Transform & Clean dá»¯ liá»‡u### 1. Clone and Setup

â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜

       â”‚```bash

       â–¼git clone <repository-url>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”cd AI_Enhance_Credit_Score_Infra

â”‚ S3 Cleaned  â”‚  â† Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½```

â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜

       â”‚ schedule (daily)### 2. Configure Variables

       â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SageMaker   â”‚  â† Huáº¥n luyá»‡n 2 models (PyTorch & scikit-learn)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  S3 Models  â”‚  â† LÆ°u trá»¯ trained models
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ auto-deploy
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SageMaker   â”‚  â† 2 Inference Endpoints
â”‚ Endpoints   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚

       â–¼terraform plan

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ API Gateway â”‚  â† REST API: /predict# Apply the infrastructure

â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜terraform apply

       â”‚```

       â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”## Configuration

â”‚   Amplify   â”‚  â† Web UI Ä‘á»ƒ tra cá»©u Ä‘iá»ƒm

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜### Core Variables (terraform.tfvars)

```

```hcl

## ğŸ“¦ YÃªu cáº§u# AWS Configuration

aws_region = "us-east-1"

### Pháº§n má»m cáº§n thiáº¿t:environment = "dev"

- **Terraform**: >= 1.0project_name = "vpbank-ai-credit-score"

- **AWS CLI**: >= 2.0

- **Python**: 3.9+# Network Configuration

- **Git**vpc_cidr = "10.0.0.0/16"

enable_nat_gateway = true

### AWS Credentials:

Cáº¥u hÃ¬nh AWS profile trong `~/.aws/credentials`:# Compute Configuration

instance_type = "t3.medium"

```iniml_instance_type = "ml.t3.medium"

[vpbank]

aws_access_key_id = YOUR_ACCESS_KEY# Database Configuration

aws_secret_access_key = YOUR_SECRET_KEYdatabase_instance_class = "db.t3.micro"

region = ap-southeast-1multi_az = false

```deletion_protection = false

```

## ğŸš€ CÃ i Ä‘áº·t

### Environment-Specific Configurations

### 1. Clone repository

#### Development Environment

```bash- Smaller instance types (t3.micro, t3.small)

git clone https://github.com/yourusername/AI_Enahance_Credit_Score_Infra.git- Single AZ RDS deployment

cd AI_Enahance_Credit_Score_Infra- Minimal backup retention

```- NAT Gateway optional (set `enable_nat_gateway = false` to save costs)



### 2. Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng#### Production Environment

- Larger instance types (t3.medium, t3.large or higher)

Táº¡o file `terraform.tfvars`:- Multi-AZ RDS deployment

- Extended backup retention

```hcl- Deletion protection enabled

aws_profile     = "vpbank"- Enhanced monitoring

aws_region      = "ap-southeast-1"

environment     = "dev"## API Usage

project_name    = "vpbank-ai-credit-score"

vpc_cidr        = "10.0.0.0/16"### Credit Score API Endpoint

ml_instance_type = "ml.t3.medium"

```The deployed infrastructure creates an API Gateway endpoint for credit scoring:



### 3. Initialize Terraform```bash

# Get the API Gateway URL from Terraform outputs

```bashterraform output api_gateway_url

terraform init

```# Example API call

curl -X POST https://your-api-gateway-url/predict \

### 4. Review vÃ  Apply  -H "Content-Type: application/json" \

  -d '{

```bash    "customer_data": {

# Xem trÆ°á»›c nhá»¯ng gÃ¬ sáº½ Ä‘Æ°á»£c táº¡o      "income": 75000,

terraform plan      "age": 32,

      "employment_length": 3,

# Táº¡o infrastructure      "loan_amount": 25000,

terraform apply      "credit_history_length": 5,

```      "existing_debt": 10000

    }

QuÃ¡ trÃ¬nh deploy máº¥t khoáº£ng **5-10 phÃºt**.  }'

```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

### Response Format

```

.```json

â”œâ”€â”€ main.tf                       # VPC, subnets, NAT gateway, security groups{

â”œâ”€â”€ variables.tf                  # Khai bÃ¡o biáº¿n  "credit_score": 725,

â”œâ”€â”€ s3.tf                         # S3 buckets (raw, cleaned, models)  "risk_category": "Low Risk",

â”œâ”€â”€ glue.tf                       # Glue jobs + Lambda trigger  "recommendation": "Approve - Excellent creditworthiness",

â”œâ”€â”€ sagemaker.tf                  # SageMaker training + deployment  "factors": {

â”œâ”€â”€ apigw.tf                      # API Gateway + Lambda inference    "income": "Positive - High income",

â”œâ”€â”€ amplify.tf                    # AWS Amplify frontend hosting    "employment": "Positive - Stable employment",

â”œâ”€â”€ quicksight.tf                 # QuickSight visualization setup
â”œâ”€â”€ config.py                     # Config cho training scripts
â”œâ”€â”€ train_traditional.py          # Script huáº¥n luyá»‡n model traditional (bao gá»“m cáº£ transaction data)
â”œâ”€â”€ train_traditional_pytorch.py  # Script huáº¥n luyá»‡n model PyTorch DNN
â”œâ”€â”€ inference_pytorch.py          # SageMaker inference handler cho PyTorch
â”œâ”€â”€ train_social.py               # Script huáº¥n luyá»‡n model social
â”œâ”€â”€ lambda_glue_starter.py        # Lambda trigger Glue jobs
â”œâ”€â”€ lambda_start_training.py      # Lambda start SageMaker training
â”œâ”€â”€ lambda_deploy_model.py        # Lambda deploy SageMaker endpoints
â”œâ”€â”€ lambda_aggregate_inference.py # Lambda aggregate 2 model predictions
â””â”€â”€ glue-scripts/
    â”œâ”€â”€ transform_traditional.py  # Glue ETL script cho traditional + transaction data
    â””â”€â”€ transform_social.py       # Glue ETL script cho social data
```

## ğŸ® HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Security Groups

### BÆ°á»›c 1: Upload dá»¯ liá»‡u thÃ´- **ALB Security Group**: HTTP/HTTPS inbound from internet

- **EC2 Security Group**: HTTP/HTTPS from ALB, SSH from VPC

Upload file CSV vÃ o S3 bucket raw:- **RDS Security Group**: MySQL/PostgreSQL from EC2 instances

- **Lambda Security Group**: Outbound internet access

```bash- **SageMaker Security Group**: HTTPS within VPC

aws s3 cp data.csv s3://vpbank-ai-credit-score-dev-raw/traditional/data.csv --profile vpbank

```### Database Layer

- **RDS MySQL 8.0**: Primary application database

Cáº¥u trÃºc folder trong raw bucket:- **ElastiCache Redis**: Session storage and caching

```- **Secrets Manager**: Database credential storage

raw/
â”œâ”€â”€ traditional/
â”‚   â””â”€â”€ data.csv
â””â”€â”€ social/
    â””â”€â”€ data.csv
```



### BÆ°á»›c 2: Tá»± Ä‘á»™ng ETL## Monitoring and Logging



Khi upload file vÃ o S3 raw bucket:### CloudWatch Integration

1. Lambda `glue_starter` Ä‘Æ°á»£c trigger tá»± Ä‘á»™ng- **Application Logs**: Centralized logging for all services

2. Glue job tÆ°Æ¡ng á»©ng cháº¡y ETL- **Metrics**: Custom metrics for credit scoring operations

3. Dá»¯ liá»‡u sáº¡ch Ä‘Æ°á»£c lÆ°u vÃ o `cleaned/` bucket- **Alarms**: Automated alerting for system health



### BÆ°á»›c 3: Huáº¥n luyá»‡n Model### Performance Monitoring

- **RDS Performance Insights**: Database performance tracking

Model Ä‘Æ°á»£c huáº¥n luyá»‡n tá»± Ä‘á»™ng theo lá»‹ch (má»—i ngÃ y 1 láº§n) hoáº·c manual trigger:- **Lambda Metrics**: Function execution metrics

- **ALB Metrics**: Load balancer health and performance

```bash

# Trigger training thá»§ cÃ´ng qua AWS CLI## Cost Optimization

aws lambda invoke \

  --function-name start-sagemaker-training \### Development Environment

  --payload '{"model_type": "traditional"}' \```hcl

  --profile vpbank \# Cost-optimized settings for development

  output.jsoninstance_type = "t3.micro"

```database_instance_class = "db.t3.micro"

ml_instance_type = "ml.t3.medium"

### BÆ°á»›c 4: Tra cá»©u Ä‘iá»ƒm tÃ­n dá»¥ngenable_nat_gateway = false

multi_az = false

Sá»­ dá»¥ng API Gateway endpoint:```



```bash### Estimated Monthly Costs (Development)

curl -X POST \- **EC2 Instances (2x t3.micro)**: ~$15

  https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/prod/predict \- **RDS (db.t3.micro)**: ~$15

  -H "Content-Type: application/json" \- **Load Balancer**: ~$20

  -d '{"customer_id": "12345"}'- **ElastiCache (cache.t3.micro)**: ~$15

```- **S3 Storage**: ~$5

- **Lambda**: ~$5 (for moderate usage)

Response:- **Other Services**: ~$10

```json

{
  "customer_id": "12345",
  "traditional_score": 720,
  "social_score": 750,
  "final_score": 735,
  "risk_level": "low"
}
```

### Data Security

## â˜ï¸ CÃ¡c dá»‹ch vá»¥ AWS- Encryption at rest for RDS and S3

- Encryption in transit for all communications

### S3 Buckets- Secrets Manager for credential management

- **raw**: Dá»¯ liá»‡u thÃ´ tá»« nguá»“n

- **cleaned**: Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ ETL### Access Control

- **models**: Trained models (joblib format)- IAM roles with principle of least privilege

- Service-specific permissions

### AWS Glue- No hardcoded credentials

- **3 Glue Jobs**: Transform data cho 3 loáº¡i model

- **Python Shell**: Xá»­ lÃ½ CSV, lÃ m sáº¡ch dá»¯ liá»‡u## Backup and Recovery

- **Output**: Timestamped CSV files

### Database Backups

### SageMaker- Automated daily backups

- **Training Jobs**: Huáº¥n luyá»‡n model sklearn LogisticRegression- Point-in-time recovery enabled

### SageMaker
- **Endpoints**: 2 inference endpoints (traditional, social)
- **Instance**: ml.t3.medium

### Lambda Functions
- `glue-starter`: Trigger Glue jobs khi cÃ³ file má»›i
- `start-training`: Khá»Ÿi Ä‘á»™ng SageMaker training
- `deploy-model`: Deploy model lÃªn endpoint
- `aggregate-inference`: Gá»i 2 endpoints vÃ  tá»•ng há»£p káº¿t quáº£

### API Gateway
- **REST API**: `/predict` endpoint
- **Method**: POST
- **Integration**: Lambda proxy

### AWS Amplify
- **Frontend Hosting**: Web UI Ä‘á»ƒ tra cá»©u Ä‘iá»ƒm
```

- **Auto Deploy**: CI/CD tá»« GitHub- Easy instance type upgrades

- **Environment Variables**: API endpoint Ä‘Æ°á»£c inject tá»± Ä‘á»™ng

## Deployment Environments

### QuickSight

- **Dashboards**: Visualize káº¿t quáº£ model### Multi-Environment Setup

- **Data Sources**: S3 manifest filesCreate separate Terraform workspaces or directories:

- **Manual Setup**: Cáº§n táº¡o dashboard qua console

```bash

## ğŸ”„ Pipeline ML# Development

terraform workspace new dev

### 1. Data Ingestionterraform apply -var-file="dev.tfvars"

```

Upload CSV â†’ S3 Raw â†’ Trigger Lambda â†’ Start Glue Job# Staging

```terraform workspace new staging

terraform apply -var-file="staging.tfvars"

### 2. ETL Process

```# Production

Glue Job â†’ Read from Raw â†’ Transform â†’ Write to Cleanedterraform workspace new prod

â”œâ”€â”€ Remove nullsterraform apply -var-file="prod.tfvars"

â”œâ”€â”€ Feature engineering```

â”œâ”€â”€ Normalize data

â””â”€â”€ Save with timestamp## Troubleshooting

```

### Common Issues

### 3. Model Training

```1. **Terraform Init Fails**

EventBridge (daily) â†’ Lambda â†’ SageMaker Training Job   - Ensure AWS credentials are configured

â”œâ”€â”€ Read from S3 Cleaned   - Check Terraform version compatibility

â”œâ”€â”€ Train sklearn model

â”œâ”€â”€ Save to S3 Models2. **Resource Creation Fails**

â””â”€â”€ Trigger deployment   - Verify AWS permissions

```   - Check resource limits in target region

   - Review Terraform error messages

### 4. Model Deployment

```3. **Application Not Accessible**

Training Complete Event â†’ Lambda â†’ Create/Update Endpoint   - Verify security group rules

â””â”€â”€ Deploy model to inference endpoint   - Check NAT Gateway configuration

```   - Review route table associations



### 5. Inference### Useful Commands

```

API Request â†’ API Gateway â†’ Lambda```bash

â”œâ”€â”€ Call Traditional Endpoint# View current infrastructure state

â”œâ”€â”€ Call Transaction Endpointterraform state list

â”œâ”€â”€ Call Social Endpoint

â””â”€â”€ Aggregate & Return Score# Get specific resource information

```terraform state show aws_instance.example



## ğŸ–¥ï¸ Giao diá»‡n ngÆ°á»i dÃ¹ng# Refresh state with actual AWS resources

terraform refresh

### Setup Amplify Frontend

# Plan changes before applying

Sau khi Terraform apply, lÃ m theo output `frontend_setup_instructions`:terraform plan -out=tfplan



1. **Táº¡o React App**:# Apply specific plan

```bashterraform apply tfplan

npx create-react-app credit-score-frontend```

cd credit-score-frontend

```## Contributing



2. **Táº¡o Component** `src/CreditScoreChecker.js`:1. Fork the repository

```javascript2. Create a feature branch

import React, { useState } from 'react';3. Make your changes

4. Test thoroughly

function CreditScoreChecker() {5. Submit a pull request

  const [customerId, setCustomerId] = useState('');

  const [result, setResult] = useState(null);## License

  const [loading, setLoading] = useState(false);

  const API_ENDPOINT = process.env.REACT_APP_API_ENDPOINT;This project is licensed under the MIT License - see the LICENSE file for details.



  const checkScore = async () => {## Support

    setLoading(true);

    try {For questions or issues:

      const response = await fetch(`${API_ENDPOINT}/predict`, {1. Check the troubleshooting section

        method: 'POST',2. Review AWS and Terraform documentation

        headers: { 'Content-Type': 'application/json' },3. Create an issue in the repository

        body: JSON.stringify({ customer_id: customerId })4. Contact the development team

      });

      const data = await response.json();---

      setResult(data);

    } catch (error) {**Note**: This infrastructure is designed for the VPBank Hackathon and should be reviewed and modified for production use according to your organization's security and compliance requirements.
      console.error('Error:', error);
      setResult({ error: error.message });
    }
    setLoading(false);
  };

  return (
    <div style={{ padding: '20px', maxWidth: '600px', margin: '0 auto' }}>
      <h1>ğŸ¦ VPBank Credit Score Checker</h1>
      <div style={{ marginBottom: '20px' }}>
        <input 
          type="text"
          value={customerId} 
          onChange={(e) => setCustomerId(e.target.value)}
          placeholder="Nháº­p Customer ID"
          style={{ padding: '10px', width: '70%', fontSize: '16px' }}
        />
        <button 
          onClick={checkScore}
          disabled={loading || !customerId}
          style={{ padding: '10px 20px', marginLeft: '10px', fontSize: '16px' }}
        >
          {loading ? 'Äang kiá»ƒm tra...' : 'Kiá»ƒm tra'}
        </button>
      </div>
      {result && (
        <div style={{ 
          padding: '20px', 
          backgroundColor: '#f5f5f5', 
          borderRadius: '8px',
          marginTop: '20px'
        }}>
          <h2>Káº¿t quáº£:</h2>
          <pre>{JSON.stringify(result, null, 2)}</pre>
        </div>
      )}
    </div>
  );
}

export default CreditScoreChecker;
```

3. **Push lÃªn GitHub**:
```bash
git init
git add .
git commit -m "Initial commit"
git remote add origin https://github.com/yourusername/credit-score-frontend.git
git push -u origin main
```

4. **Connect Amplify**:
- VÃ o [Amplify Console](https://console.aws.amazon.com/amplify/)
- Click "Connect repository"
- Chá»n GitHub â†’ Authorize â†’ Select repo
- Amplify sáº½ tá»± Ä‘á»™ng deploy!

5. **Access App**:
```
https://main.YOUR_APP_ID.amplifyapp.com
```

## ğŸ” Troubleshooting

### Lá»—i: "NoSuchBucket"
**NguyÃªn nhÃ¢n**: Terraform táº¡o bucket nhÆ°ng chÆ°a Ä‘á»£i ready.

**Giáº£i phÃ¡p**:
```bash
terraform destroy -target=aws_s3_object.glue_script_traditional
terraform apply
```

### Lá»—i: "Access Denied" khi Glue Job cháº¡y
**NguyÃªn nhÃ¢n**: IAM role thiáº¿u quyá»n S3.

**Giáº£i phÃ¡p**: Kiá»ƒm tra `glue.tf` - role cáº§n cÃ³ quyá»n:
- `s3:GetObject` trÃªn raw bucket
- `s3:PutObject` trÃªn cleaned bucket

### Lá»—i: Training Job Failed
**NguyÃªn nhÃ¢n**: KhÃ´ng cÃ³ dá»¯ liá»‡u trong cleaned bucket.

**Giáº£i phÃ¡p**:
1. Kiá»ƒm tra Glue job Ä‘Ã£ cháº¡y thÃ nh cÃ´ng chÆ°a
2. Xem log CloudWatch cá»§a Glue job
3. Upload dá»¯ liá»‡u sample Ä‘á»ƒ test

### Lá»—i: Endpoint not found
**NguyÃªn nhÃ¢n**: Model chÆ°a Ä‘Æ°á»£c deploy.

**Giáº£i phÃ¡p**:
```bash
# Trigger deployment manually
aws lambda invoke \
  --function-name deploy-sagemaker-model \
  --payload '{"model_type": "traditional"}' \
  --profile vpbank \
  output.json
```

### Lá»—i: Amplify "You should provide valid token"
**NguyÃªn nhÃ¢n**: Amplify cáº§n GitHub token Ä‘á»ƒ connect repo.

**Giáº£i phÃ¡p**: 
- Táº¡o app khÃ´ng cÃ³ repo (Ä‘Ã£ fix trong code)
- Connect repo manually qua Amplify Console

## ğŸ“Š Monitoring & Logs

### CloudWatch Logs
```bash
# Xem log Glue job
aws logs tail /aws-glue/jobs/output --follow --profile vpbank

# Xem log Lambda
aws logs tail /aws/lambda/aggregate-inference --follow --profile vpbank

# Xem log SageMaker training
aws logs tail /aws/sagemaker/TrainingJobs --follow --profile vpbank
```

### Metrics
- **S3**: Object count, bucket size
- **Lambda**: Invocations, errors, duration
- **SageMaker**: Training job status, endpoint latency
- **API Gateway**: Request count, 4xx/5xx errors

## ğŸ§¹ Dá»n dáº¹p tÃ i nguyÃªn

**âš ï¸ Cáº£nh bÃ¡o**: Lá»‡nh nÃ y sáº½ xÃ³a Táº¤T Cáº¢ tÃ i nguyÃªn!

```bash
terraform destroy
```

Náº¿u muá»‘n xÃ³a tá»«ng pháº§n:
```bash
# XÃ³a Amplify
terraform destroy -target=aws_amplify_app.credit_score_frontend

# XÃ³a SageMaker endpoints (tá»‘n phÃ­ nháº¥t)
terraform destroy -target=aws_lambda_function.deploy_model

# XÃ³a S3 buckets (cáº§n empty trÆ°á»›c)
aws s3 rm s3://vpbank-ai-credit-score-dev-raw --recursive --profile vpbank
terraform destroy -target=aws_s3_bucket.raw
```

## ğŸ’° Æ¯á»›c tÃ­nh chi phÃ­

Chi phÃ­ hÃ ng thÃ¡ng (region ap-southeast-1):

| Dá»‹ch vá»¥ | Chi phÃ­/thÃ¡ng | Ghi chÃº |
|---------|---------------|---------|
| S3 | $5-20 | TÃ¹y lÆ°á»£ng data |
| Glue | $10-50 | $0.44/DPU-hour |
| SageMaker Training | $20-100 | ml.t3.medium, cháº¡y daily |
| SageMaker Endpoints | $100-200 | 2 endpoints 24/7 |
| Lambda | $1-5 | Free tier 1M requests |
| API Gateway | $3-10 | Free tier 1M calls |
| Amplify | $0-15 | TÃ¹y traffic |
| **Tá»•ng** | **$139-400** | |

**ğŸ’¡ Tips tiáº¿t kiá»‡m**:
- DÃ¹ng SageMaker Serverless Inference thay vÃ¬ real-time endpoints
- Táº¯t endpoints khi khÃ´ng dÃ¹ng
- Sá»­ dá»¥ng S3 Intelligent-Tiering
- Set up Lambda reserved concurrency

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

## ğŸ“ License

MIT License - xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ‘¥ Team

- **VPBank AI Team**
- **Hackathon 2025**

---

**Made with â¤ï¸ for VPBank Hackathon 2025**
